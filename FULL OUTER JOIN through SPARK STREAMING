// After starting the scala-spark shell ($ spark-shell)
// Below is the API for word count in SPARK STREAMING CONTEXT through loading a data file from 2 various Local path and saving 
the result into the HDFS path.

scala> //cutting off 'scala>' for the below codes for easy copy.

import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level, Logger}
Logger.getRootLogger.setLevel(Level.WARN)
val ssc = new StreamingContext(sc, Seconds(5))
val lines1 = ssc.textFileStream("1st LFS path to directory")       // or val lines1 = ssc.socketTextStream("localhost", 9999)            
val words1 = lines1 flatMap {line => line.split(" ")}
val wordLenPairs1 = words1 map {w => (w.length, w)}
val lines2 = ssc.textFileStream("2nd LFS path to directory")    // or  val lines2 = ssc.socketTextStream("localhost", 9999)
val words2 = lines2 flatMap {line => line.split(" ")}
val wordLenPairs2 = words2 map {w => (w.length, w)}
val fullOuterJoinDS = wordLenPairs1.fullOuterJoin(wordLenPairs2)
fullOuterJoinDS.saveAsTextFiles("hdfs://localhost:8020/Path")
fullOuterJoinDS.print()

ssc.start() //This will start the spark streaming

//Open another console and copy any input file to the "1st LFS-PATH" mentioned above.
// Just put some text file under this 1st LFS path as below

$ cp sample1.txt to 1st LFS path 

//Open one more console and copy any input file to the "2nd LFS-PATH" mentioned above.
// Just put some text file under this 2nd LFS path as below

$ cp sample2.txt to 2nd LFS path 


// The ouput will be displyed into the SPARK STREAMING console which is running.
// make a note of the time of the result displayed to open that particular file into the hdfs output path
// To stop the SPARK STREAMING console pres "Ctrl+C"
// Go to the HDFS path ("hdfs://localhost:8020/PATH") to check the result 





