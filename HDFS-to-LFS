// After starting the scala-spark shell ($ spark-shell)
// Below is the API for word count in SPARK STREAMING CONTEXT through loading a data file from HDFS path and saving into LFS path.

scala> //cutting off 'scala>' for the below codes for easy copy.

import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level, Logger}
Logger.getRootLogger.setLevel(Level.WARN)
val ssc = new StreamingContext(sc, Seconds(5))
val lines = ssc.textFileStream ("hdfs://localhost:8020/PATH")
val words = lines flatMap {line => line.split(" ")}
val wordCounts = words.countByValue()
wordCounts.saveAsTextFiles("LFS-PATH")
wordCounts.print()
ssc.start()                        //This will start the spark streaming

//Open another console and copy any input file to the HDFS-PATH mentioned above.
// Just put some text file to the HDFS path

$ hadoop fs -put sample.txt into the HDFS-path

// The ouput will be displyed into the SPARK STREAMING console which is running.
// make a note of the time of the result displayed to open that particular file into the hdfs output path
// To stop the SPARK STREAMING console pres "Ctrl+C"
// Go to the HDFS path ("LFS") to check the result 
